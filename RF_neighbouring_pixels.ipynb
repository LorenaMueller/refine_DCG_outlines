{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenaMueller/refine_DCG_outlines/blob/master/RF_neighbouring_pixels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7QI3mny8Mu5"
      },
      "source": [
        "# Random Forest Classification to refine DCG outlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import ee\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter, label\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Polygon, MultiPolygon\n",
        "from skimage import measure\n",
        "from matplotlib.patches import Patch\n",
        "from shapely.geometry import shape\n",
        "from shapely.ops import unary_union\n",
        "from shapely.geometry import mapping\n",
        "import fiona"
      ],
      "metadata": {
        "id": "s5kmK7p9Xrl0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select glaciers and years\n",
        "train_year = 2016  # Always using 2016 for training\n",
        "train_glaciers = ['mauvoisin', 'zinal', 'oberaletsch', 'unteraar']  # Glaciers used for training\n",
        "test_glacier = 'zmutt'  # Glacier to test on\n",
        "test_year = [2016, 2018, 2020, 2022] # Year to test on (can be modified)\n",
        "#test_year = [2016, 2016]\n",
        "\n",
        "# ==============================================================================\n",
        "# Part I\n",
        "# ==============================================================================\n",
        "\n",
        "# Mount Google Drive (for Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths to images stored in Google Drive\n",
        "glaciers = ['zmutt', 'unteraar', 'belvedere', 'oberaletsch', 'zinal', 'mauvoisin']\n",
        "years = [2016, 2018, 2020, 2022]\n",
        "\n",
        "# Generate dynamic paths for all glaciers and years\n",
        "image_paths = {}\n",
        "for glacier in glaciers:\n",
        "    image_paths[glacier] = {}\n",
        "    for year in years:\n",
        "        image_paths[glacier][year] = f\"/content/drive/My Drive/GEE_Exports_input_layers_DCG/{glacier}_{year}_layers_drive.tif\"\n",
        "\n",
        "def read_raster(image_path):\n",
        "    dataset = gdal.Open(image_path, gdal.GA_ReadOnly)\n",
        "    if dataset is None:\n",
        "        raise ValueError(f\"Failed to open {image_path}. Check if the file exists and is readable.\")\n",
        "    image_array = dataset.ReadAsArray()\n",
        "    return np.array(image_array)  # Ensure it's a NumPy array\n",
        "\n",
        "# Load images directly from Google Drive\n",
        "original_image_data = {}\n",
        "for glacier in glaciers:\n",
        "    original_image_data[glacier] = {}\n",
        "    for year in years:\n",
        "        original_image_data[glacier][year] = read_raster(image_paths[glacier][year])\n",
        "\n",
        "#===============================================================================\n",
        "# Assuming ground truth labels are stored separately, load them\n",
        "labels_path = {\n",
        "    \"zmutt\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/zmutt_labels.tif\",\n",
        "    \"unteraar\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/unteraar_labels.tif\",\n",
        "    \"belvedere\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/belvedere_labels.tif\",\n",
        "    \"oberaletsch\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/oberaletsch_labels.tif\",\n",
        "    \"zinal\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/zinal_labels.tif\",\n",
        "    \"mauvoisin\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/mauvoisin_labels.tif\"\n",
        "}\n",
        "\n",
        "original_label_data = {}\n",
        "for key, path in labels_path.items():\n",
        "    with rasterio.open(path) as src:\n",
        "        original_label_data[key] = src.read(1)  # Assuming single-band label\n",
        "\n",
        "# Paths to shapefiles stored in Google Drive\n",
        "buffers_paths = {\n",
        "    \"zmutt\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/rgi_buffer_zmutt.shp\",\n",
        "    \"unteraar\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/rgi_buffer_unteraar.shp\",\n",
        "    \"oberaletsch\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/rgi_buffer_oberaletsch.shp\",\n",
        "    \"belvedere\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/rgi_buffer_belvedere.shp\",\n",
        "    \"zinal\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/rgi_buffer_zinal.shp\",\n",
        "    \"mauvoisin\": \"/content/drive/My Drive/GEE_Exports_input_layers_DCG/rgi_buffer_mauvoisin.shp\"\n",
        "}\n",
        "\n",
        "# Load shapefiles using geopandas\n",
        "buffers = {key: gpd.read_file(path) for key, path in buffers_paths.items()}\n",
        "\n",
        "# clip images with their corresponding buffer\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "\n",
        "# Function to clip a raster with its corresponding buffer\n",
        "def clip_raster(raster_array, raster_path, buffer_gdf):\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        # Reproject buffer to raster CRS\n",
        "        buffer_gdf = buffer_gdf.to_crs(src.crs)\n",
        "        geometries = [geom for geom in buffer_gdf.geometry]\n",
        "\n",
        "        # Get masked raster (do not fill with 0s)\n",
        "        clipped_image, clipped_transform = mask(src, geometries, crop=True, filled=False)\n",
        "\n",
        "        # Convert masked array to np.nan outside valid area\n",
        "        clipped_image = clipped_image.astype('float32')\n",
        "        clipped_image = np.where(clipped_image.mask, np.nan, clipped_image.data)\n",
        "\n",
        "        # Move bands to last axis if needed\n",
        "        if clipped_image.ndim == 3:\n",
        "            clipped_image = np.moveaxis(clipped_image, 0, -1)\n",
        "\n",
        "        clipped_meta = src.meta.copy()\n",
        "        clipped_meta.update({\n",
        "            \"height\": clipped_image.shape[0],\n",
        "            \"width\": clipped_image.shape[1],\n",
        "            \"transform\": clipped_transform\n",
        "        })\n",
        "    return clipped_image, clipped_meta\n",
        "\n",
        "def create_adaptive_buffer(rgi_gdf, dem_path, low_percentile=10, front_buffer=300, side_buffer=100):\n",
        "    with rasterio.open(dem_path) as src:\n",
        "        dem_crs = src.crs\n",
        "        rgi_gdf = rgi_gdf.to_crs(dem_crs)\n",
        "        geometries = [geom for geom in rgi_gdf.geometry]\n",
        "\n",
        "        # Clip DEM to glacier polygon\n",
        "        dem_clip, _ = mask(src, geometries, crop=True)\n",
        "        dem_data = dem_clip[0]\n",
        "        dem_data = np.where(dem_data == src.nodata, np.nan, dem_data)\n",
        "\n",
        "        # Flatten and filter out nan\n",
        "        valid_elevs = dem_data[~np.isnan(dem_data)]\n",
        "        if valid_elevs.size == 0:\n",
        "            print(\"No valid elevation data inside glacier polygon.\")\n",
        "            return None\n",
        "\n",
        "        # Threshold for lowest 10%\n",
        "        threshold = np.percentile(valid_elevs, low_percentile)\n",
        "\n",
        "        # Create binary masks for lowest 10% and the rest\n",
        "        low_area_mask = (dem_data <= threshold).astype(np.uint8)\n",
        "\n",
        "        # Convert masks to vector polygons (raster to vector)\n",
        "        shapes_low = list(measure.find_contours(low_area_mask, 0.5))\n",
        "        low_polygons = []\n",
        "        for contour in shapes_low:\n",
        "            coords = [(int(x[1]), int(x[0])) for x in contour]\n",
        "            poly = Polygon(coords)\n",
        "            if poly.is_valid:\n",
        "                low_polygons.append(poly)\n",
        "\n",
        "        # Convert original RGI glacier polygon to shapely\n",
        "        original_poly = unary_union(rgi_gdf.geometry)\n",
        "\n",
        "        # Create two buffers\n",
        "        low_area_union = unary_union(low_polygons)\n",
        "        buffer_front = low_area_union.buffer(front_buffer)\n",
        "        buffer_rest = original_poly.difference(low_area_union).buffer(side_buffer)\n",
        "\n",
        "        # Combine both\n",
        "        final_buffer = unary_union([buffer_front, buffer_rest])\n",
        "\n",
        "        return gpd.GeoDataFrame(geometry=[final_buffer], crs=dem_crs)\n",
        "\n",
        "# Dictionaries to store clipped images and labels\n",
        "image_data = {}\n",
        "label_data = {}\n",
        "\n",
        "for glacier in glaciers:\n",
        "  image_data[glacier] = {}  # Initialize glacier-level dictionary\n",
        "  label_data[glacier], _ = clip_raster(original_label_data[glacier], labels_path[glacier], buffers[glacier])\n",
        "\n",
        "for glacier in glaciers:\n",
        "  for year in years:\n",
        "        # Clip the image\n",
        "        image_data[glacier][year], _ = clip_raster(original_image_data[glacier][year], image_paths[glacier][year], buffers[glacier])\n",
        "\n",
        "# for glacier in glaciers:\n",
        "#     label_data[glacier] = {}  # Initialize dictionary for years\n",
        "#     for year in years:\n",
        "#         label_data[glacier][year], _ = clip_raster(original_label_data[glacier], labels_path[glacier], buffers[glacier])\n",
        "\n",
        "# Only using the specified bands for training and testing\n",
        "selected_bands = ['lstNir', 'lstNir_superPixelMeans', 'edges', 'smooth', 'ndwi', 'ndsi', 'ndvi', 'slope', 'aspect', 'elevation', 'gradient', 'TPI', 'glacierInventory', 'inSAR', 'normalizedLst', 'coherence', 'VH']\n",
        "bands = selected_bands\n",
        "\n",
        "# Mapping band names to indices in the images (these should match the band order in the images)\n",
        "band_indices = {\n",
        "    #'lstNir': 0,\n",
        "    #'ndri': 1,\n",
        "    #'lstNir_superPixelMeans': 2,\n",
        "    #'edges': 3,\n",
        "    #'smooth': 4,\n",
        "    'ndwi': 5,\n",
        "    'ndsi': 6,\n",
        "    'ndvi': 7,\n",
        "    'slope': 8,\n",
        "    #'aspect': 9,\n",
        "    #'elevation': 10,\n",
        "    #'gradient': 11,\n",
        "    #'TPI': 12,\n",
        "    #'glacierInventory': 13,\n",
        "    #'inSAR': 14,\n",
        "    'normalizedLst': 15,\n",
        "    'coherence': 16,\n",
        "    'VH': 17\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdVqBt17aK9R",
        "outputId": "deed52de-50d0-4ad5-9b01-70db4a5b017c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def extract_with_neighbors(image_array, label_array, selected_bands, band_indices):\n",
        "    \"\"\"\n",
        "    Extract pixel values and their 4 neighbors for selected bands, flatten to table format.\n",
        "    Includes **all** pixels by padding the edges (mode='reflect').\n",
        "    Only includes pixels with valid (non-NaN) labels.\n",
        "    \"\"\"\n",
        "    # Pad top/bottom by 1, left/right by 1, leave band axis unchanged\n",
        "    padded = np.pad(image_array, ((1, 1), (1, 1), (0, 0)), mode=\"reflect\")\n",
        "\n",
        "    h, w, _ = image_array.shape\n",
        "    data_list = []\n",
        "\n",
        "    # Loop over original pixel coordinates\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            label = label_array[y, x]\n",
        "            if np.isnan(label):\n",
        "                continue\n",
        "\n",
        "            feature_dict = {\"label\": int(label)}\n",
        "            for band in selected_bands:\n",
        "                if band not in band_indices:\n",
        "                    continue\n",
        "                idx = band_indices[band]\n",
        "\n",
        "                # center is at padded[y+1, x+1]\n",
        "                yc, xc = y + 1, x + 1\n",
        "                feature_dict[f\"{band}_center\"] = padded[yc,   xc,   idx]\n",
        "                feature_dict[f\"{band}_top\"]    = padded[yc-1, xc,   idx]\n",
        "                feature_dict[f\"{band}_bottom\"] = padded[yc+1, xc,   idx]\n",
        "                feature_dict[f\"{band}_left\"]   = padded[yc,   xc-1, idx]\n",
        "                feature_dict[f\"{band}_right\"]  = padded[yc,   xc+1, idx]\n",
        "\n",
        "            data_list.append(feature_dict)\n",
        "\n",
        "    return pd.DataFrame(data_list)\n"
      ],
      "metadata": {
        "id": "d83GYl7OfoLf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nPIAKy-cfLO",
        "outputId": "114b8335-787d-4d50-8416-45284a09bc5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-bfcabaa2f099>:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  feature_dict = {\"label\": int(label)}\n",
            "<ipython-input-21-bfcabaa2f099>:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  feature_dict = {\"label\": int(label)}\n",
            "<ipython-input-21-bfcabaa2f099>:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  feature_dict = {\"label\": int(label)}\n",
            "<ipython-input-21-bfcabaa2f099>:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  feature_dict = {\"label\": int(label)}\n",
            "<ipython-input-21-bfcabaa2f099>:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  feature_dict = {\"label\": int(label)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Band usage frequency:\n",
            "ndwi: 17\n",
            "ndsi: 17\n",
            "ndvi: 17\n",
            "slope: 17\n",
            "normalizedLst: 16\n",
            "coherence: 16\n",
            "VH: 16\n",
            "band combinations [['ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right'], ['ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right'], ['coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right'], ['slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right'], ['slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right'], ['ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right'], ['ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right'], ['coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right'], ['slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right'], ['slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right'], ['ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right'], ['ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right'], ['ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right'], ['ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right'], ['VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right'], ['VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right'], ['coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right'], ['slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right', 'VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right'], ['ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right', 'normalizedLst_center', 'normalizedLst_top', 'normalizedLst_bottom', 'normalizedLst_left', 'normalizedLst_right', 'coherence_center', 'coherence_top', 'coherence_bottom', 'coherence_left', 'coherence_right'], ['VH_center', 'VH_top', 'VH_bottom', 'VH_left', 'VH_right', 'ndwi_center', 'ndwi_top', 'ndwi_bottom', 'ndwi_left', 'ndwi_right', 'ndsi_center', 'ndsi_top', 'ndsi_bottom', 'ndsi_left', 'ndsi_right', 'ndvi_center', 'ndvi_top', 'ndvi_bottom', 'ndvi_left', 'ndvi_right', 'slope_center', 'slope_top', 'slope_bottom', 'slope_left', 'slope_right']]\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 3\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 6\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 9\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 12\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 15\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 18\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 21\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 24\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 27\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 30\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 33\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 36\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 39\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 42\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 45\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n",
            "max ensemble result 48\n",
            "y_pred_full [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Prepare training and testing data\n",
        "#selected_bands_rf = ['slope', 'coherence', 'ndsi', 'ndwi', 'ndvi', 'VH', 'normalizedLst']\n",
        "# Define bands and suffixes\n",
        "base_bands = ['ndwi', 'ndsi', 'ndvi', 'slope', 'normalizedLst', 'coherence', 'VH']\n",
        "suffixes = ['center', 'top', 'bottom', 'left', 'right']\n",
        "\n",
        "# Generate selected band names (flattened list)\n",
        "selected_bands_rf = [f\"{band}_{suffix}\" for band in base_bands for suffix in suffixes]\n",
        "\n",
        "X_train_dfs = []\n",
        "for glacier in train_glaciers:\n",
        "    image = image_data[glacier][train_year]\n",
        "    label = label_data[glacier]\n",
        "    df = extract_with_neighbors(image, label, base_bands, band_indices)  # No need to pass band_indices\n",
        "    df = df[selected_bands_rf + [\"label\"]]\n",
        "\n",
        "    #print(\"df columns:\", df.columns)\n",
        "    X_train_dfs.append(df)\n",
        "\n",
        "train_df = pd.concat(X_train_dfs, ignore_index=True)\n",
        "X_train = train_df.drop(columns=\"label\").values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "\n",
        "# Update band_indices for future reference\n",
        "#band_indices = {band: idx for idx, band in enumerate(train_df.drop(columns=\"label\").columns)}\n",
        "\n",
        "\n",
        "# # Collect testing samples\n",
        "# mask = (~np.isnan(label_data[test_glacier][year])).squeeze()\n",
        "# X_test = image_data[test_glacier][year][:, :, list(band_indices.values())]\n",
        "# X_test = X_test[mask].reshape(-1, len(band_indices))\n",
        "# y_test = label_data[test_glacier][year].squeeze()[mask].ravel()\n",
        "\n",
        "ensemble_results = {}\n",
        "smoothed_images = {}\n",
        "smoothed_images_flat = {}\n",
        "accuracies_by_year = {}\n",
        "reports_by_year = {}\n",
        "\n",
        "for year in test_year:\n",
        "   #print(f\"\\n=== Processing year: {year} ===\")\n",
        "\n",
        "    # === (1) Generate X_test and y_test with neighbor features ===\n",
        "    image = image_data[test_glacier][year]\n",
        "    label = label_data[test_glacier]\n",
        "    test_df = extract_with_neighbors(image, label, base_bands, band_indices)\n",
        "    #test_df = test_df[selected_bands_rf + [\"label\"]]\n",
        "\n",
        "    #print(\"test_df columns:\", test_df.columns)\n",
        "    X_test = test_df.drop(columns=\"label\").values\n",
        "    y_test = test_df[\"label\"].values\n",
        "\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "    from sklearn.utils import resample\n",
        "\n",
        "    # Stratified sampling to reduce size (if needed)\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=2, random_state=42)\n",
        "    for train_idx, _ in sss.split(X_train, y_train):\n",
        "        X_tmp, y_tmp = X_train[train_idx], y_train[train_idx]\n",
        "\n",
        "    # Convert labels to int if not already\n",
        "    y_tmp = y_tmp.astype(int)\n",
        "\n",
        "    # Oversampling\n",
        "    X_0 = X_tmp[y_tmp == 0]\n",
        "    X_1 = X_tmp[y_tmp == 1]\n",
        "\n",
        "    X_1_up, y_1_up = resample(X_1, y_tmp[y_tmp == 1], replace=True, n_samples=len(X_0), random_state=42)\n",
        "\n",
        "    X_train_balanced = np.vstack((X_0, X_1_up))\n",
        "    y_train_balanced = np.hstack((np.zeros(len(X_0)), np.ones(len(X_0))))\n",
        "\n",
        "    # Check the class distribution in the training set before oversampling\n",
        "    unique, counts = np.unique(y_train, return_counts=True)\n",
        "    class_distribution = dict(zip(unique, counts))\n",
        "\n",
        "    # Similarly for the balanced training data\n",
        "    unique_balanced, counts_balanced = np.unique(y_train_balanced, return_counts=True)\n",
        "    class_distribution_balanced = dict(zip(unique_balanced, counts_balanced))\n",
        "\n",
        "    # print(\"Class distribution in the training data (before oversampling):\", class_distribution)\n",
        "    # print(\"Class distribution in the balanced training data (after oversampling):\", class_distribution_balanced)\n",
        "    # print(\"Test set class distribution:\", dict(zip(*np.unique(y_test, return_counts=True))))\n",
        "\n",
        "    # band_combinations = [[\n",
        "    #     f\"{band}_{suffix}\"\n",
        "    #     for band in ['ndwi', 'ndsi', 'ndvi', 'slope', 'normalizedLst', 'coherence', 'VH']\n",
        "    #     for suffix in ['center', 'top', 'bottom', 'left', 'right']\n",
        "    #     ]]\n",
        "\n",
        "    import random\n",
        "    from collections import defaultdict\n",
        "    random.seed(42)\n",
        "\n",
        "    base_bands = ['ndwi', 'ndsi', 'ndvi', 'slope', 'normalizedLst', 'coherence', 'VH']\n",
        "    suffixes = ['center', 'top', 'bottom', 'left', 'right']\n",
        "    n_combinations = 20\n",
        "    bands_per_combo_range = (5, 7)  # number of base bands per combination\n",
        "\n",
        "    # Track how many times each base band is used\n",
        "    band_usage_count = defaultdict(int)\n",
        "    band_combinations = []\n",
        "\n",
        "    for _ in range(n_combinations):\n",
        "        # Prefer bands that have been used the least\n",
        "        sorted_bands = sorted(base_bands, key=lambda b: band_usage_count[b])\n",
        "        num_bands = random.randint(*bands_per_combo_range)\n",
        "\n",
        "        selected_bands = sorted_bands[:num_bands]\n",
        "        for band in selected_bands:\n",
        "            band_usage_count[band] += 1\n",
        "\n",
        "        full_band_set = [f\"{band}_{suffix}\" for band in selected_bands for suffix in suffixes]\n",
        "        band_combinations.append(full_band_set)\n",
        "\n",
        "    # Optional: Check usage balance\n",
        "    print(\"Band usage frequency:\")\n",
        "    for band in base_bands:\n",
        "        print(f\"{band}: {band_usage_count[band]}\")\n",
        "    print('band combinations', band_combinations)\n",
        "\n",
        "    # Result: band_combinations is your list of 20 diverse input sets\n",
        "\n",
        "    # Extract the actual list of bands from the first (and only) combination\n",
        "    bands_flat = band_combinations[0]\n",
        "    band_indices_new = {name: idx for idx, name in enumerate(bands_flat)}\n",
        "\n",
        "    # Initialize an array to store classification results\n",
        "    ensemble_result = np.zeros_like(label_data[test_glacier], dtype=int)\n",
        "    #ensemble_result = np.zeros(mask.shape, dtype=int)\n",
        "    ensemble_results[year] = ensemble_result.copy()\n",
        "\n",
        "# ==============================================================================\n",
        "# Part II\n",
        "# ==============================================================================\n",
        "\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from collections import defaultdict\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.inspection import permutation_importance\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Storage\n",
        "    mdi_importance_dict = defaultdict(list)\n",
        "    perm_importance_records = []\n",
        "    accuracies = []\n",
        "\n",
        "    label_mask = (~np.isnan(label_data[test_glacier])).squeeze()\n",
        "    ensemble_result = np.zeros(label_mask.shape, dtype=int)\n",
        "\n",
        "\n",
        "    for bands in band_combinations:\n",
        "        #print(f\"Processing bands: {bands}\")\n",
        "\n",
        "        band_indices_selected = [\n",
        "            band_indices_new[b] for b in bands if band_indices_new[b] < X_train_balanced.shape[1]\n",
        "        ]\n",
        "\n",
        "        if not band_indices_selected:\n",
        "            print(f\"Skipping bands {bands} as none were valid.\")\n",
        "            continue\n",
        "\n",
        "        # print(\"band_indices_selected:\", band_indices_selected)\n",
        "        # print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "        # print(f\"Selected bands indices: {band_indices_selected}\")\n",
        "        # print(f\"Feature names: {[f'Band_{i}' for i in band_indices_selected]}\")\n",
        "\n",
        "        X_train_selected = X_train_balanced[:, band_indices_selected]\n",
        "        X_test_selected = X_test[:, band_indices_selected]\n",
        "\n",
        "        clf = RandomForestClassifier(n_estimators=50, random_state=42, max_samples=6000)\n",
        "        clf.fit(X_train_selected, y_train_balanced)\n",
        "\n",
        "        y_pred_full = clf.predict(X_test_selected).ravel()\n",
        "        print('y_pred_full', y_pred_full)\n",
        "        accuracy = accuracy_score(y_test, y_pred_full)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        # print(\"label_mask shape:\", label_mask.shape)\n",
        "        # print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "        # Flatten the label mask to get indices of valid positions\n",
        "        # Step 1: Flatten the label mask\n",
        "        # label_mask_flat = label_mask.flatten()\n",
        "\n",
        "        # # Step 2: Get indices of valid labels\n",
        "        # valid_indices = np.where(label_mask_flat)[0]\n",
        "\n",
        "        # # Step 3: Extract corresponding X_test rows\n",
        "        # X_test_valid = X_test[valid_indices]\n",
        "\n",
        "        # # Step 4: Identify rows with no NaNs\n",
        "        # non_nan_mask = ~np.isnan(X_test_valid).any(axis=1)\n",
        "\n",
        "        # # Step 5: Filter out NaNs\n",
        "        # X_test_selected = X_test_valid[non_nan_mask]\n",
        "\n",
        "        # # Step 6: Also update the valid_indices accordingly\n",
        "        # valid_indices = valid_indices[non_nan_mask]\n",
        "\n",
        "        # print('y test', np.unique(y_test, return_counts=True))\n",
        "        # print('y pred', np.unique(y_pred_full, return_counts=True))\n",
        "        # print(\"y_test shape:\", y_test.shape)\n",
        "        # print(\"y_pred shape:\", y_pred_full.shape)\n",
        "\n",
        "        # print(\"X_test shape:\", X_test.shape)\n",
        "        # print(\"X_test_selected shape:\", X_test_selected.shape)\n",
        "        # print(\"valid_indices:\", valid_indices.shape)\n",
        "        # print('valid_mask', valid_mask.shape)\n",
        "        # print(\"Any NaNs in X_test?\", np.isnan(X_test).any())\n",
        "\n",
        "        # Make sure the sizes align\n",
        "        #assert y_pred_full.shape[0] == X_test_selected.shape[0]\n",
        "\n",
        "        # 1. Create a flat ensemble array (same as before)\n",
        "        ensemble_result_flat = ensemble_result.flatten()\n",
        "\n",
        "        # 2. Create the valid mask (where label_mask is non-zero)\n",
        "        label_mask_flat = label_mask.flatten()\n",
        "        valid_mask = label_mask_flat != 0  # Adjust the condition if necessary\n",
        "\n",
        "        # 3. Make sure the shapes match between valid_mask and y_pred_full\n",
        "        assert np.sum(valid_mask) == y_pred_full.shape[0], \"Number of valid positions doesn't match number of predictions.\"\n",
        "\n",
        "        # 4. Only update the valid positions in ensemble_result_flat with y_pred_full\n",
        "        ensemble_result_flat[valid_mask] += y_pred_full.astype(int)\n",
        "\n",
        "        # 5. Reshape back to 2D\n",
        "        ensemble_result = ensemble_result_flat.reshape(label_mask.shape)\n",
        "\n",
        "        # # Create an ensemble result flat array to update\n",
        "        # ensemble_result_flat = ensemble_result.flatten()\n",
        "        # ensemble_result_flat[valid_indices] += y_pred_full.astype(int)\n",
        "\n",
        "        # # Reshape back to 2D\n",
        "        # ensemble_result = ensemble_result_flat.reshape(label_mask.shape)\n",
        "\n",
        "        # print(\"=== Debug Info ===\")\n",
        "        # print(f\"y_pred.shape: {y_pred_full.shape}\")  # Should be (N,)\n",
        "        # print(f\"label_mask.shape: {label_mask_flat.shape}\")  # Should be (M,) — possibly with M > N\n",
        "        # print(f\"np.sum(label_mask): {np.sum(label_mask_flat)}\")  # Should be N to pass the assert\n",
        "        # print(f\"type(label_mask): {type(label_mask_flat)}, dtype: {label_mask_flat.dtype}\")\n",
        "        # print(f\"Example values in label_mask: {label_mask_flat[:10]}\")\n",
        "        assert y_pred_full.shape[0] == np.sum(label_mask_flat)\n",
        "        ensemble_result[label_mask] += y_pred_full.astype(int)\n",
        "\n",
        "\n",
        "        if y_pred_full.shape[0] == np.sum(label_mask_flat):\n",
        "          ensemble_result[label_mask] += y_pred_full.astype(int)\n",
        "        else:\n",
        "            print(\"Mismatch between prediction and mask! Skipping vote accumulation.\")\n",
        "            print(\"y_pred.shape[0]:\", y_pred_full.shape[0])\n",
        "            print(\"np.sum(label_mask):\", np.sum(label_mask_flat))\n",
        "\n",
        "        # MDI Importances (Mean Decrease in Impurity)\n",
        "        for band, importance in zip(bands, clf.feature_importances_):\n",
        "            mdi_importance_dict[band].append(importance)\n",
        "\n",
        "        # Permutation Importances (Test Set)\n",
        "        perm_result = permutation_importance(\n",
        "            clf, X_test_selected, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
        "        )\n",
        "\n",
        "        for i, band in enumerate(bands):\n",
        "            for score in perm_result.importances[i]:\n",
        "                perm_importance_records.append({\n",
        "                    \"Band\": band,\n",
        "                    \"Permutation Importance\": score\n",
        "                })\n",
        "        print('max ensemble result', np.max(ensemble_result))\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1. MDI (Mean Decrease in Impurity) Plot\n",
        "    # -----------------------------\n",
        "    # mdi_df = pd.DataFrame({\n",
        "    #     band: pd.Series(scores) for band, scores in mdi_importance_dict.items()\n",
        "    # })\n",
        "    # mdi_mean = mdi_df.mean().sort_values(ascending=True)\n",
        "\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "    # mdi_mean.plot(kind='barh')\n",
        "    # plt.title(\"Random Forest Feature Importances (MDI)\")\n",
        "    # plt.xlabel(\"Mean Decrease in Impurity\")\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Permutation Importance Boxplot\n",
        "    # -----------------------------\n",
        "    perm_df = pd.DataFrame(perm_importance_records)\n",
        "\n",
        "    import seaborn as sns  # Optional, prettier plots\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Convert Band to categorical with sorted order\n",
        "    ordered_bands = (\n",
        "        perm_df.groupby(\"Band\")[\"Permutation Importance\"]\n",
        "        .median()\n",
        "        .sort_values()\n",
        "        .index\n",
        "    )\n",
        "    perm_df[\"Band\"] = pd.Categorical(perm_df[\"Band\"], categories=ordered_bands, ordered=True)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    perm_df.boxplot(\n",
        "        column=\"Permutation Importance\",\n",
        "        by=\"Band\",\n",
        "        vert=False,\n",
        "        grid=False,\n",
        "        patch_artist=True\n",
        "    )\n",
        "    plt.title(\"Permutation Importances (Test Set, All Combinations)\")\n",
        "    plt.suptitle(\"\")\n",
        "    plt.xlabel(\"Decrease in Accuracy Score\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Extract base band name (e.g., 'ndvi' from 'ndvi_top', 'ndvi_center' etc.)\n",
        "    perm_df[\"BaseBand\"] = perm_df[\"Band\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
        "\n",
        "    # Group by BaseBand, collecting all permutation scores for each variation\n",
        "    grouped_df = perm_df.groupby(\"BaseBand\")[\"Permutation Importance\"].apply(list).reset_index()\n",
        "\n",
        "    # Explode the list to get one value per row (for boxplotting)\n",
        "    exploded_df = grouped_df.explode(\"Permutation Importance\")\n",
        "\n",
        "    # Sort base bands by median importance\n",
        "    ordered_basebands = (\n",
        "        exploded_df.groupby(\"BaseBand\")[\"Permutation Importance\"]\n",
        "        .median()\n",
        "        .sort_values(ascending=False)\n",
        "        .index\n",
        "    )\n",
        "    exploded_df[\"BaseBand\"] = pd.Categorical(exploded_df[\"BaseBand\"], categories=ordered_basebands, ordered=True)\n",
        "\n",
        "    # Plot grouped boxplot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(\n",
        "        x=\"Permutation Importance\",\n",
        "        y=\"BaseBand\",\n",
        "        data=exploded_df,\n",
        "        orient=\"h\",\n",
        "        palette=\"viridis\"\n",
        "    )\n",
        "\n",
        "    plt.title(\"Permutation Importances (Grouped by Base Band)\")\n",
        "    plt.xlabel(\"Decrease in Accuracy Score\")\n",
        "    plt.ylabel(\"Band\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ==============================================================================\n",
        "    # Part III\n",
        "    # ==============================================================================\n",
        "\n",
        "    # Plot Intermediate Classification Result (before thresholding)\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    im = ax.imshow(ensemble_result, interpolation='nearest', cmap=\"viridis\")  # Using 'viridis' for better visualization\n",
        "\n",
        "    # Remove axis labels and ticks\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    # Add colorbar to show accumulated votes\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.6)\n",
        "    cbar.set_label(\"Accumulated Votes\")\n",
        "\n",
        "    plt.title(f\"Classification sum {year}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Compute threshold (90th percentile - 1)\n",
        "    from skimage.filters import threshold_otsu\n",
        "\n",
        "    threshold = 1#threshold_otsu(ensemble_result[mask])  # Ensure it's on the masked values\n",
        "    binary_classification = (ensemble_result >= threshold).astype(int)\n",
        "\n",
        "    # Apply Gaussian Smoothing\n",
        "    smoothed_image = gaussian_filter(binary_classification.astype(float), sigma=1)\n",
        "    smoothed_image = np.round(smoothed_image)\n",
        "\n",
        "    smoothed_images[year] = smoothed_image.copy()\n",
        "\n",
        "    # Calculate accuracy of the final classification result\n",
        "    # Ensure smoothed_image matches the shape of the original mask\n",
        "    smoothed_image_flat = smoothed_image[label_mask].ravel()\n",
        "    smoothed_images_flat[year] = smoothed_image_flat.copy()\n",
        "\n",
        "\n",
        "    # Compare the smoothed classification result with the ground truth labels\n",
        "    #y_true = label_data[test_region].squeeze()[mask].ravel()  # Flatten ground truth labels\n",
        "    y_true = label_data[test_glacier].squeeze()[label_mask].ravel()\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_true, smoothed_image_flat)\n",
        "    accuracies_by_year[year] = accuracy\n",
        "    print(f\"Final Accuracy (after thresholding and smoothing) {year}: {accuracy:.4f}\")\n",
        "\n",
        "    # Print classification report for final classification\n",
        "    report = classification_report(y_true, smoothed_image_flat)\n",
        "    reports_by_year[year] = report\n",
        "    print(f\"Classification Report for final result {year}:\")\n",
        "    print(report)\n",
        "\n",
        "    # Plot Smoothed Classification Result\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    im = ax.imshow(smoothed_image, interpolation='nearest', cmap=\"binary\")\n",
        "\n",
        "    # Remove axis labels and ticks\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    # Correcting label_data shape by squeezing out any extra dimensions\n",
        "    label_2d = label_data[test_glacier].squeeze()\n",
        "\n",
        "    # Overlay ground truth labels as contours\n",
        "    contours = measure.find_contours(label_2d, level=0.5)\n",
        "    for contour in contours:\n",
        "        ax.plot(contour[:, 1], contour[:, 0], linewidth=1, color=\"red\", label=\"SGI2016 Outline\")\n",
        "\n",
        "    # Add Legend\n",
        "    legend_elements = [\n",
        "        Patch(facecolor=\"black\", edgecolor=\"black\", label=\"Classified Region\"),\n",
        "        Patch(facecolor=\"white\", edgecolor=\"red\", label=\"SGI2016 outline\")\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc=\"lower left\", fontsize=10)\n",
        "\n",
        "    # Add Scale Bar (1 pixel = 30 m --> 30 * 33.3 ~ 1km)\n",
        "    scalebar_length = 33.3\n",
        "    scalebar_x = smoothed_image.shape[1] * 0.90  # Position: 90% from left\n",
        "    scalebar_y = smoothed_image.shape[0] * 0.95  # Position: 95% from top\n",
        "    ax.plot([scalebar_x, scalebar_x + scalebar_length], [scalebar_y, scalebar_y],\n",
        "            color='black', linewidth=3)\n",
        "    ax.text(scalebar_x + scalebar_length / 2, scalebar_y - 5, \"1 km\", ha=\"center\", fontsize=10, color=\"black\")\n",
        "\n",
        "    # Add North Arrow\n",
        "    north_x = smoothed_image.shape[1] * 0.95\n",
        "    north_y = smoothed_image.shape[0] * 0.05\n",
        "    ax.annotate(\"N\", xy=(north_x, north_y), xytext=(north_x, north_y + 30),\n",
        "                arrowprops=dict(facecolor=\"black\", width=3, headwidth=8),\n",
        "                fontsize=12, color=\"black\", ha=\"center\")\n",
        "\n",
        "    plt.title(f\"Smoothed Classification Result {year}\")\n",
        "    plt.show()\n",
        "\n",
        "    # for year in years:\n",
        "    #     print(f\"Processing year: {year}\")  # Debugging statement\n",
        "\n",
        "for year in test_year:\n",
        "    print(f\"\\n==== Summary for Year: {year} ====\")\n",
        "    print(f\"Accuracy: {accuracies_by_year[year]:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(reports_by_year[year])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "1jRn85YLloalZFmTHlXFLXcxmTe0Iwm3t",
      "authorship_tag": "ABX9TyNLGCHcoXMpbJuxkHPIOJta",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}